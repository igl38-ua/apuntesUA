# Resumen

- **Propiedades emergentes no funcionales** (fiabilidad, disponibilidad, mantenibilidad, escalabilidad, robustez) se validan en aceptaci√≥n porque s√≥lo tienen sentido sobre el **sistema completo**.
- Deben traducirse a **criterios de aceptaci√≥n medibles**; se emplean m√©tricas como **MTTF, MTTR, MTBF** (fiabilidad), MTTR (disponibilidad / mantenibilidad) y **throughput** o transacciones ‚àï seg para escalabilidad.
- **Pruebas de carga** confirman el rendimiento dentro de los l√≠mites de dise√±o; **pruebas de estr√©s** superan dichos l√≠mites para evaluar robustez; **pruebas estad√≠sticas** calculan fiabilidad a partir de un **perfil operacional** (probabilidad de cada clase de entrada).
- Proceso general de rendimiento:
    1. **Identificar** criterios y cuantificarlos.
    2. **Dise√±ar** los casos a partir del perfil operacional.
    3. **Preparar** un entorno realista.
    4. **Automatizar** (p.ej. **JMeter**) y **ejecutar**.
    5. **Analizar** resultados y refinar.
- **JMeter** automatiza pruebas de aceptaci√≥n/presi√≥n:
    - Plan = **Test Plan + Thread Groups + Samplers**.
    - Cada **hilo** simula un usuario; el **ramp-up** reparte su creaci√≥n.
    - **Samplers** env√≠an peticiones; su orden real se gobierna con **Controladores l√≥gicos**.
    - **Timers** introducen pausas; **Assertions** validan respuestas; **Listeners** recogen m√©tricas (average, percentile 90, min/max, % error, **throughput**, KB/s).
- Las decisiones de **arquitectura** condicionan el rendimiento; por ello las pruebas no funcionales deben iniciarse en las primeras iteraciones y repetirse de forma continua.

___
# S09 - Pruebas de aceptaci√≥n 2
## Contexto y objetivos

Las pruebas de aceptaci√≥n se sit√∫an al final de la cadena **unidad ‚Üí integraci√≥n ‚Üí sistema ‚Üí aceptaci√≥n**.

___
## Propiedades emergentes no funcionales

| Propiedad          | Descripci√≥n                                                             | Ejemplo de criterio    |
| ------------------ | ----------------------------------------------------------------------- | ---------------------- |
| **Fiabilidad**     | Probabilidad de funcionar sin fallos durante un periodo y entorno dados | ‚â• 90 % en 24 h         |
| **Disponibilidad** | Tiempo durante el cual el sistema presta servicio al usuario            | 24/7 con MTTR ‚â§ 15 min |
| **Mantenibilidad** | Facilidad de aplicar cambios correctivos, adaptativos o perfectivos     | MTTR de cambio < 4 h   |
| **Escalabilidad**  | Capacidad de mantener tiempos de respuesta al aumentar usuarios         | 10 000 usuarios, ‚â§ 2 s |
| **Robustez**       | Capacidad de recuperarse y seguir operando ante fallos                  | Sin ca√≠da con picos √ó2 |

> Estas propiedades s√≥lo son evaluables sobre el sistema completo y constituyen la base de los **criterios de aceptaci√≥n no funcionales**.

___
## M√©tricas asociadas

- Los criterios de aceptaci√≥n deben incluir propiedades emergentes "cuantificables".

|M√©trica|Uso|F√≥rmula / unidad|
|---|---|---|
|**MTTF** (Mean Time To Failure)|Fiabilidad|Tiempo medio hasta fallo|
|**MTTR** (Mean Time To Repair)|Disponibilidad / mantenibilidad|Tiempo medio de reparaci√≥n|
|**MTBF** (Mean Time Between Failures)|Fiabilidad global|MTTF + MTTR|
|**Throughput**|Rendimiento / escalabilidad|peticiones √∑ seg|
|**Kb/s**|Rendimiento de red|KB transferidos √∑ seg|

> Evitar frases ambiguas como ‚Äútiempo razonable‚Äù: si no podemos medir, **no podremos aceptar**.

___
## Tipos de pruebas para validar rendimiento

### Pruebas de carga

Confirman que el sistema mantiene el tiempo de respuesta requerido bajo el **volumen previsto** de usuarios y transacciones. Ej.: ‚Äú‚â§ 2 s con 10 000 usuarios concurrentes‚Äù.
### Pruebas de estr√©s

Incrementan gradualmente la carga **por encima** del l√≠mite de dise√±o (p.ej. > 300 TPS) para observar el punto de fallo y la capacidad de recuperaci√≥n. Valoran **robustez**.
### Pruebas estad√≠sticas

1. **Perfil operacional**: clasificar entradas y asignarles probabilidad.
2. **Generaci√≥n de datos** seg√∫n esa distribuci√≥n (n√∫meros aleatorios 1‚Äì99 ‚Üí clase).
3. **Ejecuci√≥n y medici√≥n** de fallos/tiempo hasta fallo para estimar fiabilidad.

```text
Perfil operacional (ejemplo)
Clase C1 ‚Üí 50 % ‚Üí RNG 1-49
Clase C2 ‚Üí 15 % ‚Üí RNG 50-64
...
```

![item](../img/ppss/ejemplos%20de%20pruebas.png)

![item](../img/ppss/ejemplo%20de%20generaci√≥n%20de%20pruebas.png)

___
## Proceso de pruebas no funcionales

1. **Identificar** criterios cuantificables (tiempos, tasas error, throughput‚Ä¶).
2. **Dise√±ar** casos basados en escenarios reales de uso (perfil operacional).
3. **Preparar** un entorno de pruebas representativo de producci√≥n.
4. **Automatizar y ejecutar** con herramientas (JMeter, ‚Ä¶).
5. **Analizar** resultados y refinar arquitectura o configuraci√≥n.

> Las decisiones arquitect√≥nicas influyen directamente en estas propiedades: **no postergar** las pruebas de rendimiento al final del proyecto.

___
## Automatizaci√≥n con Apache JMeter

### Elementos fundamentales

|Elemento|Rol|Icono JMeter|
|---|---|---|
|**Test Plan**|Ra√≠z; agrupa todo el driver|üìÑ|
|**Thread Group**|Define usuarios, ramp-up, bucle|üë•|
|**Sampler**|Env√≠a peticiones (HTTP, JDBC‚Ä¶)|üåê|
|**Logic Controller**|Altera orden / condicionales|üîÅ|
|**Timer**|Introduce pausas|‚è±|
|**Assertion**|Comprueba la respuesta|‚úÖ|
|**Listener**|Registra y muestra resultados|üìä|

### Flujo de ejecuci√≥n interno

- Se repite por **hilo** e **iteraci√≥n**.

```
Configuration Elements
‚Üì
Pre-Processors
‚Üì
Timers
‚Üì
Sampler
‚Üì
Post-Processors
‚Üì
Assertions
‚Üì
Listeners
```

### Configuraci√≥n de un Thread Group

- **N√∫mero de hilos** = usuarios simulados.
- **Ramp-up** = segundos para levantar todos los hilos.
- **Loop count** = n¬∫ de repeticiones; ‚Äúforever‚Äù para bucle infinito.

```xml
<!-- fragmento .jmx ilustrativo -->
<ThreadGroup num_threads="100" ramp_time="60" loops="10">
	...
</ThreadGroup>
```

![item](../img/ppss/hilos%20de%20ejecuci√≥n.png)
### Samplers y Controladores

- **HTTP Request** sampler: URL, m√©todo, par√°metros.
- **Loop Controller**: repite sus hijos _n_ veces.
- **Interleave Controller**: ejecuta un hijo distinto en cada iteraci√≥n (ej.: b√∫squedas ‚ÄúA‚Äù, ‚ÄúB‚Äù).

![item](../img/ppss/samplers%201.png)

![item](../img/ppss/sampler%202.png)
### Timers comunes

|Timer|Uso|
|---|---|
|**Constant Timer**|Pausa fija entre peticiones|
|**Uniform Random**|Pausa aleatoria uniforme|
|**Gaussian Random**|Pausa con distribuci√≥n gaussiana|

> Para que un timer afecte a **un solo** sampler, a√±√°delo como hijo; si est√° al mismo nivel, afectar√° a todos los samplers de la rama.

![item](../img/ppss/timers.png)
### Assertions

A√±adir al menos una aserci√≥n por sampler para verificar c√≥digo HTTP, contenido esperado, tama√±o de respuesta, etc.

![item](../img/ppss/aserciones.png)
### Listeners y m√©tricas

- **Aggregate Report**, **Summary Report**, **Graph Results**, **Response-Time Graph**‚Ä¶
- Datos por etiqueta de muestra: _#Samples, Average, Median, 90 % Line, Min, Max, %Error, Throughput, KB/s_.
- **Throughput** = peticiones / segundo (capacidad del servidor).

![item](../img/ppss/listeners.png)

___
## Buenas pr√°cticas de rendimiento con JMeter

1. Ejecutar JMeter desde **otra m√°quina** que no sea el SUT.
2. Repetir los escenarios durante **largos periodos** para detectar degradaci√≥n.
3. Usar **m√∫ltiples instancias** JMeter para cargas extremas (modo distribuido).
4. Basar los tests en **casos de uso reales** y perfil operacional.
5. Controlar el entorno: ancho de banda, uso exclusivo del servidor, etc.
